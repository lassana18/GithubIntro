{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "30th Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1J6MCXRDxnLyHOwir_sRKBon1sUgCMQ05",
      "authorship_tag": "ABX9TyPPCBwVx5Bq2dgZVuit2ElL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pafernannapi18/GithubIntro/blob/main/30th_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problem 1] Looking back on the scratch"
      ],
      "metadata": {
        "id": "rdPCQckw12mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement deep learning from the scratches review, we needed:\n",
        "\n",
        "\n",
        "To initialize the weights;\n",
        "\n",
        "\n",
        "An epoch loop;\n",
        "\n",
        "\n",
        "To code the Activation Functions\n",
        "\n",
        "\n",
        "To decide:\n",
        "- the learning rate;\n",
        "- sizes;\n",
        "- number of nodes"
      ],
      "metadata": {
        "id": "OivPewr62xgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 2] Consider the correspondence between scratch and TensorFlow"
      ],
      "metadata": {
        "id": "aEJJVakX3D8m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iHB0xpiK10h8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffb5aba9-324d-4303-ea17-31c0d4337fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 37.3006, val_loss : 85.7779, acc : 0.750, val_acc : 0.375\n",
            "Epoch 1, loss : 0.0000, val_loss : 0.2778, acc : 1.000, val_acc : 0.938\n",
            "Epoch 2, loss : 0.1729, val_loss : 2.0768, acc : 1.000, val_acc : 0.688\n",
            "Epoch 3, loss : 3.6089, val_loss : 5.4849, acc : 0.750, val_acc : 0.688\n",
            "Epoch 4, loss : 5.7678, val_loss : 14.7125, acc : 0.750, val_acc : 0.375\n",
            "Epoch 5, loss : 2.2236, val_loss : 4.6686, acc : 0.750, val_acc : 0.688\n",
            "Epoch 6, loss : 6.4915, val_loss : 15.1588, acc : 0.750, val_acc : 0.375\n",
            "Epoch 7, loss : 0.3088, val_loss : 4.2003, acc : 0.750, val_acc : 0.688\n",
            "Epoch 8, loss : 1.0615, val_loss : 5.6314, acc : 0.750, val_acc : 0.562\n",
            "Epoch 9, loss : 0.0000, val_loss : 1.7155, acc : 1.000, val_acc : 0.875\n",
            "test_acc : 0.900\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "#Getting the Iris dataset and preparing the variables and target\n",
        "iris =\"/content/drive/MyDrive/Iris.csv\"\n",
        "df = pd.read_csv(iris)\n",
        "\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "# Convert labels to numbers\n",
        "y[y=='Iris-versicolor'] = 0\n",
        "y[y=='Iris-virginica'] = 1\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Further split into train and val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    # Iterator to get the mini-batch\n",
        "    \n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Configure hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Determine the form of the arguments to be passed to the computational graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train's mini-batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    \"\"\"\n",
        "    A simple three-layer neural network\n",
        "    \"\"\"\n",
        "    # Declaring weights and biases\n",
        "\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    \n",
        "    # tf.add and + are equivalent\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "\n",
        "# Load the network structure \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "\n",
        "# Optimization methods\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimation results\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "\n",
        "# Index value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Run a computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop per epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above, we see that the Weights initialization is done through tf.Variable(tf.random_normal); Adam is used as an Optimizer and the activation function passes through tf.nn.relu."
      ],
      "metadata": {
        "id": "6QEe9b2a4D9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problem 3] Create a model of Iris using all three types of objective variables\n",
        "\n",
        "Utilization of all three types of objective variables"
      ],
      "metadata": {
        "id": "2Pg7idXK4G1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the Iris dataset and preparing the variables and target\n",
        "iris =\"/content/drive/MyDrive/Iris.csv\"\n",
        "df = pd.read_csv(iris)\n",
        "\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "# Convert labels to numbers\n",
        "y[y=='Iris-setosa'] = 0\n",
        "y[y=='Iris-versicolor'] = 1\n",
        "y[y=='Iris-virginica'] = 2\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Further split into train and val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# One-hot encoding of correct label value\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    # Iterator to get the mini-batch\n",
        "    \n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Configure hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "# Determine the form of the arguments to be passed to the computational graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train's mini-batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    \"\"\"\n",
        "    A simple three-layer neural network\n",
        "    \"\"\"\n",
        "    # Declaring weights and biases\n",
        "    \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        " \n",
        "# Load the network structure \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "\n",
        "# Optimization methods\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimation results\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "\n",
        "# Index value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Run a computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop per epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DsfwiUQ4FDx",
        "outputId": "23a7579c-5e72-4d7f-fdbe-feac99813705"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 97.6563, val_loss : 43.4188, acc : 0.333, val_acc : 0.708\n",
            "Epoch 1, loss : 10.6289, val_loss : 30.2793, acc : 0.833, val_acc : 0.625\n",
            "Epoch 2, loss : 2.4611, val_loss : 6.5836, acc : 0.833, val_acc : 0.708\n",
            "Epoch 3, loss : 1.1232, val_loss : 8.9156, acc : 0.833, val_acc : 0.708\n",
            "Epoch 4, loss : 0.0001, val_loss : 5.0399, acc : 1.000, val_acc : 0.792\n",
            "Epoch 5, loss : 0.0000, val_loss : 1.2839, acc : 1.000, val_acc : 0.833\n",
            "Epoch 6, loss : 0.0000, val_loss : 2.1826, acc : 1.000, val_acc : 0.875\n",
            "Epoch 7, loss : 0.0000, val_loss : 0.8446, acc : 1.000, val_acc : 0.917\n",
            "Epoch 8, loss : 0.0000, val_loss : 1.1032, acc : 1.000, val_acc : 0.833\n",
            "Epoch 9, loss : 0.0000, val_loss : 0.9606, acc : 1.000, val_acc : 0.833\n",
            "test_acc : 0.967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problem 4] Creating a model of House Prices\n",
        "\n",
        "Creating a model by using the House Prices"
      ],
      "metadata": {
        "id": "GIadBQaS74-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#Getting the Iris dataset and preparing the variables and target\n",
        "data =\"/content/drive/MyDrive/Train dataset/train.csv\"\n",
        "df = pd.read_csv(data)\n",
        "\n",
        "y = df[\"SalePrice\"]\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "y = np.log(y)\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Further split into train and val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    # Iterator to get the mini-batch\n",
        "    \n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Configure hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Determine the form of the arguments to be passed to the computational graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train's mini-batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    \"\"\"\n",
        "    A simple three-layer neural network\n",
        "    \"\"\"\n",
        "    # Declaring weights and biases    \n",
        "    \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "\n",
        "# Load the network structure \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
        "\n",
        "# Optimization methods\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "# Estimation results\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "y_pred = logits\n",
        "\n",
        "# Initialize the variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Run a computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    loss_list = []\n",
        "    val_loss_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop per epoch\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
        "        loss_list.append(loss)\n",
        "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
        "        val_loss_list.append(val_loss)    \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
        "    print(\"test_mse : {:.3f}\".format(loss))\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.plot(loss_list, label='loss')\n",
        "    plt.plot(val_loss_list, label='val_loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nc-biRXR8BDo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "45ef8e5c-efe3-4c20-da4c-c5e21d4be946"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 561183.4375, val_loss : 571833.1875\n",
            "Epoch 1, loss : 106472.0312, val_loss : 108632.0156\n",
            "Epoch 2, loss : 159678.5312, val_loss : 154941.4844\n",
            "Epoch 3, loss : 136526.5625, val_loss : 128664.9141\n",
            "Epoch 4, loss : 212398.2188, val_loss : 208207.1094\n",
            "Epoch 5, loss : 47682.1445, val_loss : 48939.9805\n",
            "Epoch 6, loss : 33004.3867, val_loss : 33207.7695\n",
            "Epoch 7, loss : 21897.3379, val_loss : 21849.3418\n",
            "Epoch 8, loss : 26644.7715, val_loss : 30009.2676\n",
            "Epoch 9, loss : 109126.5078, val_loss : 115122.3516\n",
            "test_mse : 109126.508\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problem 5] Creating a MNIST model\n",
        "Creation of the model that classifies the MNIST"
      ],
      "metadata": {
        "id": "st5D0LSL_FNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "#Flatten\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "# Type conversion, normalization\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
        "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:])\n",
        "y_test_one_hot = enc.fit_transform(y_test[:])\n",
        "\n",
        "#Split into train and validations sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \n",
        "    # Iterator to get the mini-batch\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Configure hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "# Determine the form of the arguments to be passed to the computational graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train's mini-batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    \"\"\"\n",
        "    A simple three-layer neural network\n",
        "    \"\"\"\n",
        "    # Declaring weights and biases  \n",
        "    \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "\n",
        "# Load the network structure                                \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "\n",
        "# Optimization methods\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimation results\n",
        "max_Y = (tf.argmax(Y, 1))\n",
        "max_Y_pred = tf.argmax(logits, 1)\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#initialize the variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Run a computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop per epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "    \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QB43iBv_Gim",
        "outputId": "2da3d79a-6d12-46ba-d3b9-b1c967734616"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 0, loss : 0.5380, val_loss : 0.9060, acc : 0.900, val_acc : 0.840\n",
            "Epoch 1, loss : 0.5285, val_loss : 0.4251, acc : 0.800, val_acc : 0.894\n",
            "Epoch 2, loss : 0.3950, val_loss : 0.3257, acc : 0.900, val_acc : 0.914\n",
            "Epoch 3, loss : 0.3957, val_loss : 0.2727, acc : 0.900, val_acc : 0.930\n",
            "Epoch 4, loss : 0.4957, val_loss : 0.3305, acc : 0.900, val_acc : 0.923\n",
            "Epoch 5, loss : 0.4235, val_loss : 0.3171, acc : 0.800, val_acc : 0.931\n",
            "Epoch 6, loss : 0.3592, val_loss : 0.2705, acc : 0.900, val_acc : 0.938\n",
            "Epoch 7, loss : 0.3800, val_loss : 0.3383, acc : 0.800, val_acc : 0.930\n",
            "Epoch 8, loss : 0.6598, val_loss : 0.3455, acc : 0.800, val_acc : 0.928\n",
            "Epoch 9, loss : 0.4887, val_loss : 0.3511, acc : 0.800, val_acc : 0.935\n",
            "test_acc : 0.938\n"
          ]
        }
      ]
    }
  ]
}